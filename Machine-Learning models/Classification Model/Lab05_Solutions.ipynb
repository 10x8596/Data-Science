{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> COMP2420/COMP6420 - Introduction to Data Management, Analysis and Security</h1>\n",
    "\n",
    "<h2 align='center'> Lab 05 - Data Analysis: Classification </h2>\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim\n",
    "Our aim in this lab is:\n",
    "- Understand and implement a logistic regression model for classification\n",
    "- Understand and implement a k-Nearest Neighbour model for classification\n",
    "- Compare the two classification techniques and understand the capabilities and pitfalls of each\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "- L03: Demonstrate basic knowledge and understanding of descriptive and predictive data analysis methods, optimization and search, and knowledge representation.\n",
    "- L04: Formulate and extract descriptive and predictive statistics from data\n",
    "- L05: Analyse and interpret results from descriptive and predictive data analysis\n",
    "- L06: Apply their knowledge to a given problem domain and articulate potential data analysis problems\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Before starting this lab, we suggest you complete the following:\n",
    "- Watch the lectures this week\n",
    "- Complete Lab04 in particular and become familiar with Scikit-Learn's modules\n",
    "\n",
    "\n",
    "The following functions may be useful for this lab:\n",
    "\n",
    "| Function                     | Description |\n",
    "| ---:                         | :---        |\n",
    "| `LogisticRegression()`, `KNeighborsClassifier()` | create an instance of a classification module |\n",
    "| `LabelEncoder()`, `StandardScaler()` | create an instance of a pre-processing module |\n",
    "\n",
    "We have not included functions described in previous labs (especially those used to fit, predict and score models) as we expect you to be familiar with those.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression     # Logistic Regression\n",
    "from sklearn.neighbors import KNeighborsClassifier      # k-Nearest Neighbours\n",
    "from sklearn.preprocessing import LabelEncoder          # encooding variables\n",
    "from sklearn.preprocessing import StandardScaler        # encooding variables\n",
    "from sklearn.model_selection import train_test_split    # testing our models\n",
    "from sklearn.metrics import confusion_matrix            # scoring\n",
    "\n",
    "import matplotlib.pyplot as plt    # plotting, if you need it\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Not-So-Linear Regression\n",
    "\n",
    "In 1912, the British passenger liner *RMS Titanic* hit an iceberg and sank. Many of the passengers died, and the event is considered to be one of the deadliest marine disasters. Today, we'll be analysing the statistics of the passengers to understand the factors that led to their survivability. We'd like to **predict (or rather, classify) whether a passenger would live or die** depending on factors such as age, gender and passenger class. Here's a description of the columns:\n",
    "\n",
    "| Name           | Description |\n",
    "| ---:           | :---        |\n",
    "| `PassengerId`  | an arbitrary ID assigned to each passenger |\n",
    "| `Survived`     | status of passenger's survival<br>(`0`=No, `1`=Yes) |\n",
    "| `Pclass`       | passenger's ticket class<br>(`1`=Upper, `2`=Middle, `3`=Lower) |\n",
    "| `Name`         | full title and name of passenger |\n",
    "| `Sex`          | gender of passenger |\n",
    "| `Age`          | age of passenger<br>fractional if less than 1, xx.5 if estimated |\n",
    "| `SibSp`        | number of siblings and spouses aboard<br>brother / sister / stepbrother / stepsister / husband / wife |\n",
    "| `Parch`        | number of parents and children aboard<br>mother / father / daughter / son / stepdaughter / stepson |\n",
    "| `Ticket`       | ticket ID |\n",
    "| `Fare`         | passenger fare ($) |\n",
    "| `Cabin`        | cabin number |\n",
    "| `Embarked`     | port of embarkation<br>(`C`=Cherbourg, `Q`=Queenstown, `S`=Southampton) |\n",
    "\n",
    "This data was collected from <a href=\"https://www.kaggle.com/c/titanic\">Kaggle</a>.\n",
    "\n",
    "In previous labs, we've given you a lot of guidance on how to deal with data - missing values, choosing your columns, etc. This time we'll give you the freedom (and responsibility) of deciding this for yourself. In making these decisions, feel free to consult classmates, tutors, previous labs and lectures, and online research as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Preparing the Data\n",
    "First, we'll need to **import the data**. The data is in the file `data/titanic.csv`; save it to an object named `titanic` and view the first ten rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "5            6         0       3   \n",
      "6            7         0       1   \n",
      "7            8         0       3   \n",
      "8            9         1       3   \n",
      "9           10         1       2   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "5                                   Moran, Mr. James    male   NaN      0   \n",
      "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
      "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
      "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
      "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "5      0            330877   8.4583   NaN        Q  \n",
      "6      0             17463  51.8625   E46        S  \n",
      "7      1            349909  21.0750   NaN        S  \n",
      "8      2            347742  11.1333   NaN        S  \n",
      "9      0            237736  30.0708   NaN        C  \n"
     ]
    }
   ],
   "source": [
    "def import_data(url):\n",
    "    \"\"\" \n",
    "    Import data from an address.\n",
    "            Parameters:\n",
    "                    url (string): File path for the data.\n",
    "            Returns:\n",
    "                    data (DataFrame): A dataframe of the data.\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    data = pd.read_csv(url)\n",
    "    return data\n",
    "\n",
    "def first_ten_rows_inspection(data):\n",
    "    \"\"\" \n",
    "    Inspect the first ten rows. \n",
    "            Parameters:\n",
    "                    data (DataFrame): A dataframe of the data.\n",
    "            Returns:\n",
    "                    None.\n",
    "    \"\"\"\n",
    "    print(data.head(10))\n",
    "    \n",
    "titanic = import_data(\"data/titanic.csv\")\n",
    "first_ten_rows_inspection(titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your first impressions from this data? You may wish to do some further data exploration (for example, finding missing values, the distribution of data, descriptive statistics) to help you understand what you're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls per column:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TODO: explore the data\n",
    "print(titanic.describe())\n",
    "\n",
    "# plot pclass\n",
    "classes = np.sort(titanic[\"Pclass\"].unique())\n",
    "heights = [len(titanic[titanic[\"Pclass\"]==pclass]) for pclass in classes]\n",
    "plt.bar(classes, heights, align='center')\n",
    "plt.xticks(np.arange(len(classes))+1, classes)\n",
    "plt.title(\"Dist of Passenger Class\")\n",
    "plt.show()\n",
    "\n",
    "# plot fare\n",
    "plt.hist(titanic[\"Fare\"])\n",
    "plt.title(\"Dist of Fare\")\n",
    "plt.show()\n",
    "\n",
    "# you can plot other variables as you wish\n",
    "\n",
    "print(\"Nulls per column:\")\n",
    "print(titanic.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping in mind that classifying survivability is the goal here, work with the data to make it useful for a classification model. If you're unsure what to do, it's quite similar to prediction (as you would have done in the last lab), so treat this exercise as if you're about to do prediction.\n",
    "\n",
    "For example: Which columns should you drop? What should you do when you encounter an entry with a missing value? Do you need to recode any columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not yet! There are some missing values in the data.\n"
     ]
    }
   ],
   "source": [
    "def data_preprocessing(data):\n",
    "    \"\"\" \n",
    "    Prepare your data - drop unneccersary columns, deal with entries with a missing value, etc.\n",
    "            Parameters:\n",
    "                    Original data.\n",
    "            Returns:\n",
    "                    Preprocessed data.\n",
    "    \"\"\"\n",
    "    new_titanic = titanic[['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Survived']]\n",
    "    \n",
    "    new_titanic = new_titanic.dropna()\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    recoded_Sex = le.fit_transform(new_titanic['Sex'])\n",
    "    new_titanic.loc[:, 'Sex'] = recoded_Sex\n",
    "    \n",
    "    return new_titanic\n",
    "    \n",
    "processed_titanic = data_preprocessing(titanic)\n",
    "if titanic.isnull().sum().sum() == 0:\n",
    "    print('Yeah! You have successfully preprocessed your data.')\n",
    "else:\n",
    "    print('Not yet! There are some missing values in the data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we won't tell you whether you've prepared the data correctly or not, so **any issues you encounter later may be a result of an incorrect decision at this stage**. Feel free to bounce ideas off others before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Logistics\n",
    "We'll be using two classification techniques in this lab. The first is **Logistic Regression** - not to be confused with linear regression from last lab. It's quite a complicated technique, but we'll try to avoid a lot of the theory and teach you how to use it. The main thing you'll need to know is that logistic regression is a powerful tool, but is only really useful for two-class classification. It's perfect for this exercise, because survivability can take either 0 or 1.\n",
    "\n",
    "Have a look at the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">documentation for Scikit-Learn's Logistic Regression module</a>; you'll need to refer to it for this exercise. Alternatively, you can run `help(LogisticRegression)` to view the documentation through Jupyter (which would be useful for in-lab examinations).\n",
    "\n",
    "First **split your data** into training and testing (with 80% training and 20% testing). Then **create an instance of the `LogisticRegression()` tool**, and **fit the data** using the instance and save it to an object called `logres_model`. When creating the instance, use `solver=lbfgs` and specify `max_iter=1000`. This specifies the method used for optimisation of the model, and allows more iterations for the model to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, no machine learning model is useful if you can't make predictions with it. Using the test set that you created earlier, **calculate the train and test scores** of the model. To increase the score, try adding or removing predictors and compare with classmates to see what they got. Note that the scores here are no longer $R^2$, but **mean accuracy**. We'll explain this in more detail later in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(571, 5)\n",
      "(143, 5)\n",
      "(571,)\n",
      "(143,)\n",
      "Intercept : [4.73589363]\n",
      "Attributes Coefficients Dictionary:  {'Pclass': -1.127675674590574, 'Sex': -2.4238055626297323, 'Age': -0.037009007836955606, 'SibSp': -0.34593066491978497, 'Fare': 0.002478019033171337}\n",
      "Train Score: 0.7933450087565674; Test Score: 0.8391608391608392\n"
     ]
    }
   ],
   "source": [
    "def data_split(data):\n",
    "    \"\"\" \n",
    "    Split your data with 80% training and 20% testing.\n",
    "            Parameters:\n",
    "                    Original Data.\n",
    "            Returns:\n",
    "                    Train data;\n",
    "                    Test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = data[['Pclass', 'Sex', 'Age', 'SibSp', 'Fare']]\n",
    "    y = data['Survived']\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(X,y,test_size = 0.2, random_state=2420)\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "train_x, test_x, train_y, test_y = data_split(processed_titanic)\n",
    "[print(d.shape) for d in [train_x, test_x, train_y, test_y]]\n",
    "\n",
    "def logistic_regression(data):\n",
    "    \"\"\" \n",
    "    Split your data; Create an instance of the LogisticRression() tool; fit the data;\n",
    "            Parameters:\n",
    "                    Original Data.\n",
    "            Returns:\n",
    "                    Logistic Regression Instance;\n",
    "                    Intercept;\n",
    "                    Coef_dict (dict): A dictionary with the keys to be attribute names, \n",
    "                    and the values to be the corresponding coefficients from your model;\n",
    "                    Train_score (rounding to two decimal places);\n",
    "                    Test_score (rounding to two decimal places).\n",
    "    \"\"\"\n",
    "    train_x, test_x, train_y, test_y = data_split(data)\n",
    "    \n",
    "    lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    logres_model = lr.fit(train_x, train_y)\n",
    "    intercept = logres_model.intercept_\n",
    "    coef = {k:co for k, co in zip(train_x.columns,logres_model.coef_[0])}\n",
    "\n",
    "    \n",
    "    train_score = logres_model.score(train_x, train_y)\n",
    "    test_score = logres_model.score(test_x, test_y)\n",
    "    \n",
    "    \n",
    "    return logres_model, intercept, coef, train_score, test_score\n",
    "\n",
    "logres_model, intercept, coef, train_score, test_score = logistic_regression(processed_titanic)\n",
    "print(\"Intercept :\", intercept)\n",
    "print(\"Attributes Coefficients Dictionary: \", coef)\n",
    "print(f\"Train Score: {train_score}; Test Score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the model, use `logres_model.intercept_` and `logres_model.coef_` to get the coefficients assigned to each column. You'll need to match the order of the coefficients to the order of the predictors when you fit the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Result Analysis\n",
    "\n",
    "As with prediction, a positive coefficient indicates that a higher predictor leads to a higher probability of the target variable being 1. For example, you might find that the coefficient for `Pclass` is negative - this is because a lower `Pclass` value (eg. First Class) leads to a higher chance of survival. **Unlike linear regression, this doesn't translate directly**; a coefficient of 1.5 does not mean a probability increase of 150%. Instead, it is a **transformation** of the original linear regression formula. If you're interested in learning more, we encourage you to do some online research. As a starting point, try <a href=\"https://machinelearningmastery.com/logistic-regression-for-machine-learning/\">this link</a>. It's likely that you'll study logistic regression in much further detail in future courses at ANU.\n",
    "\n",
    "Please answer the following questions in the text box:\n",
    "1. **Find the coefficient for each predictor and describe its effect** (positive, negative, or insignificant) on survivability. You can (and should) also compare coefficients between predictors (eg. age has a stronger effect than class on survivability).\n",
    "2. Do you think that Logistic Regression is a suitable model for the titanic data? Is it overfitting or underfitting? Why? You should consider looking at the training and test scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TODO: for each predictor, write their associated coefficient and describe its meaning\n",
    "\n",
    "Passenger class has coef -1.112, so Lower Class passengers had lower survival than Upper Class passengers (significant effect)\n",
    "Sex has coef -2.519; females had higher survival (significant effect)\n",
    "Age, SibSp and Parch have small (but negative) effect on surival; those who are younger, and have fewer relatives, had slightly higher survival\n",
    "Fare has coef 0.00389, which seems insignificant but the variable also has a much higher range (also likely correlated with Pclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You died :(\n"
     ]
    }
   ],
   "source": [
    "# TODO: predict survivability for a new person\n",
    "# lower class, male, 20yo, 0 relatives on-board, $10 fare\n",
    "pred = logres_model.predict(np.array([[3, 1, 20, 0, 10]]))\n",
    "print(\"You survived!\" if pred==1 else \"You died :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: In The Neighbourhood\n",
    "The second classification technique we'll learn is **k-Nearest Neighbour**, often shortened to kNN. The general idea (at least, for 1-Nearest Neighbour), is that you make the model memorise all the training data, and when you get a new point for prediction, you match it to the \"most similar\" point in the training set and give it the same label. For kNN, we compare it to the k most similar training points and give it the most common label amongst those points.\n",
    "\n",
    "#### 2.1 Scaling Data, Not Fish\n",
    "Consider two features, `Pclass` and `Age`, and two points:\n",
    "1. `Pclass`=1, `Age`=40, `Survived`=1\n",
    "2. `Pclass`=3, `Age`=20, `Survived`=0\n",
    "\n",
    "You've likely found that `Pclass` is far more important predictor than `Age` - passengers with First Class tickets were more likely to board lifeboats, and thus had a higher chance of surviving. However, `Pclass` has a range of 1-3 while `Age` has a range of 0-80. For k-Nearest Neighbours, this means that comparing to a point like `Pclass`=1, `Age`=20, thus `Survived`=1, the first point above would have a distance of 20 while the second point would have a distance of 2.\n",
    "\n",
    "This is why we need to **scale data**, so that the range of a predictor doesn't affect its distance. To do this, we can use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\">StandardScaler module in Scikit learn</a>.\n",
    "\n",
    "Because we don't know what the testing data looks like, it would be improper to scale depending on the range of the testing data. So, implement the following:\n",
    "1. Using a StandardScaler instance, **fit and transform only the training data**, naming the transformed data `train_scaled`. \n",
    "2. Then, using the same instance, **transform the testing data separately** (without re-fitting) and name it `test_scaled`.\n",
    "\n",
    "We print the mean and variance of `train_scaled` and `test_scaled` for you. Even you get it worked properly, you might find that, for the training set, they aren't *exactly* 0 and 1, but any difference is insignificant. You should have found a different mean and variance for the scaled testing set; this is because we used the distribution of the training set to scale the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be sure it worked, **find the mean and variance** of `train_scaled` and `test_scaled`. You might find that, for the training set, they aren't *exactly* 0 and 1, but any difference is insignificant. You should have found a different mean and variance for the scaled testing set; this is because we used the distribution of the training set to scale the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled train data mean:  9.1462156004146e-17\n",
      "Scaled train data variance:  0.9999999999999999\n",
      "Scaled test data mean:  0.008296831414526232\n",
      "Scaled test data variance:  0.9069520596919367\n"
     ]
    }
   ],
   "source": [
    "# TODO: scale data\n",
    "def data_scaling(train_x, test_x):\n",
    "    \"\"\" \n",
    "    fit and transform the given data.\n",
    "            Parameters:\n",
    "                    Train data;\n",
    "                    Test data.\n",
    "            Returns:\n",
    "                    Scaled train data;\n",
    "                    Scaled test data.\n",
    "    \"\"\"\n",
    "    ss = StandardScaler()\n",
    "    ss_model = ss.fit(train_x)\n",
    "    train_x_scaled = ss_model.transform(train_x)\n",
    "    test_x_scaled = ss_model.transform(test_x)\n",
    "    return train_x_scaled, test_x_scaled\n",
    "\n",
    "train_x, test_x, train_y, test_y = data_split(processed_titanic)\n",
    "train_x_scaled, test_x_scaled = data_scaling(train_x, test_x)\n",
    "print('Scaled train data mean: ', train_x_scaled.mean())\n",
    "print('Scaled train data variance: ', train_x_scaled.var())\n",
    "print('Scaled test data mean: ', test_x_scaled.mean())\n",
    "print('Scaled test data variance: ', test_x_scaled.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the types of `train_scaled` and `test_scaled` - you'll notice that the scaling module doesn't return a Pandas DataFrame. So that we can apply the same machine learning modules as we have before, convert both of these objects back to Pandas DataFrames, and ensure that their columns are named appropriately. *Hint: renaming columns can be done without explicitly typing out each column name.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: convert to DataFrames and name columns\n",
    "train_x_scaled = pd.DataFrame(train_x_scaled, columns=processed_titanic.columns[:-1])\n",
    "test_x_scaled = pd.DataFrame(test_x_scaled, columns=processed_titanic.columns[:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Getting To Know Your Neighbours\n",
    "Look at the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">documentation for Scikit-Learn's kNN Classifier</a>, or use `help(KNeighborsClassifier)`.\n",
    "\n",
    "Let's continue using the Titanic dataset for predicting survival. Just like you did for logistic regression, \n",
    "1. **Create an instance of the KNeighborsClassifier**. For now, set `n_neighbors=5` (i.e. $k=5$). „ÄÅ\n",
    "2. Then fit the model and name it `knn_model`. As the instance expects the target variable to have integer values, give it the non-scaled target column for the `y` argument.\n",
    "3. Now find the **training and testing scores** for this model (rounding to two decimal places). Compare this testing score to the testing score you obtained for logistic regression earlier, and also compare your score with other students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.861646234676007\n",
      "Test score:  0.7622377622377622\n"
     ]
    }
   ],
   "source": [
    "# TODO: fit KNN classifier\n",
    "def knn(data):\n",
    "    \"\"\" \n",
    "    Split and scale your data using what you wrote before; Create an instance of the LogisticRression() tool; fit the data;\n",
    "            Parameters:\n",
    "                    Original Data.\n",
    "            Returns:\n",
    "                    KNN Instance;\n",
    "                    Train_score (rounding to two decimal places);\n",
    "                    Test_score (rounding to two decimal places).\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    processed_titanic = data_preprocessing(data)\n",
    "    train_x, test_x, train_y, test_y = data_split(processed_titanic)\n",
    "    train_x_scaled, test_x_scaled = data_scaling(train_x,test_x)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_model = knn.fit(train_x_scaled, train_y)\n",
    "    \n",
    "    train_score = knn_model.score(train_x_scaled, train_y)\n",
    "    test_score = knn_model.score(test_x_scaled, test_y)\n",
    "    \n",
    "    return knn_model, train_score, test_score\n",
    "\n",
    "knn_model, train_score_knn, test_score_knn = knn(titanic) \n",
    "print(\"Training Score:\", train_score_knn)\n",
    "print(\"Test score: \", test_score_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 How Big Should Our Neighbourhood Be?\n",
    "Earlier, we used `n_neighbors=5` when creating the kNN instance. Try increasing or decreasing this parameter and see how it affects the model performance. Note that `k` is a hyperparameter of knn, so to avoid overfitting to the test set, we need:\n",
    "1. firstly **create a validation set**. \n",
    "2. **Find the best `k` on the validation set and evaluate the model with the best `k` on the test set**. You can either adjust the code you wrote previously, or copy it here and adjust it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a validation set\n",
    "train_x_scaled, val_x_scaled, train_y, val_y = train_test_split(train_x_scaled,train_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Training Score: 0.993421052631579 Validation Score:  0.8\n",
      "2 Training Score: 0.8881578947368421 Validation Score:  0.782608695652174\n",
      "3 Training Score: 0.8947368421052632 Validation Score:  0.8\n",
      "5 Training Score: 0.8706140350877193 Validation Score:  0.8\n",
      "7 Training Score: 0.8552631578947368 Validation Score:  0.782608695652174\n",
      "9 Training Score: 0.8442982456140351 Validation Score:  0.7652173913043478\n",
      "15 Training Score: 0.8333333333333334 Validation Score:  0.7739130434782608\n",
      "31 Training Score: 0.8026315789473685 Validation Score:  0.7739130434782608\n",
      "51 Training Score: 0.7785087719298246 Validation Score:  0.7913043478260869\n",
      "456 Training Score: 0.6030701754385965 Validation Score:  0.5478260869565217\n",
      "The best k is 5 and the best val score is 0.8000\n"
     ]
    }
   ],
   "source": [
    "# TODO: play around with parameters and find the best model on validation set\n",
    "best_k = -1\n",
    "best_score = -1\n",
    "for k in [1,2,3,5,7,9,15,31,51,train_x_scaled.shape[0]]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)    # just change the n_neighbors parameter\n",
    "    knn_model = knn.fit(train_x_scaled, train_y) # scaled X, un-scaled y\n",
    "    train_score = knn.score(train_x_scaled, train_y)\n",
    "    val_score = knn.score(val_x_scaled, val_y)\n",
    "    print(k, \"Training Score:\", train_score, \"Validation Score: \", val_score)\n",
    "    # find the best k\n",
    "    if best_score <= val_score:\n",
    "        best_score = val_score\n",
    "        best_model = knn_model\n",
    "        besk_k = k\n",
    "\n",
    "print(f'The best k is {besk_k} and the best val score is {best_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Test Score: 0.8042\n"
     ]
    }
   ],
   "source": [
    "# TODO: evalate the best model using the holdout test set.\n",
    "print(\"Best Model Test Score: {:.4f}\".format(best_model.score(test_x_scaled, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the best choice of parameter value? Try not to fine-tune it too much (as this can lead to overfitting in your model, and you shouldn't be using the testing score to adjust your model). What would happen when we set `n_neighbors=N`, where `N` is the number of entries in the training set? Alternatively, what about `n_neighbors=1`? *Hint: think about what a small difference in the predictors values of a new point would cause.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: understand the n_neighbors parameter\n",
    "\n",
    "For n_neighbors=N, the model just assigns the most common target value to any new testing point - so it's really just going \"Well, most people died, so I'll assume anyone else I hear about died too\". Try `train_y.value_counts()[0]/sum(train_y.value_counts())`.\n",
    "\n",
    "For n_neighbors=1, it only finds the closest training point, so a small change in its predictor values might lead to a different neighbour, and thus the opposite target prediction.\n",
    "\n",
    "As n_neighbors increases, the training score will decrease (think about why this is), but the testing score will fluctuate. We recommend an odd number around >=5 and <=15, depending on the size and variation in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Classifying Flowers\n",
    "It's likely that you found a higher testing score, and a much more accurate training score, for logistic regression than for kNN. However, as we've mentioned before, (binary) logistic regression has a major pitfall: it can only classify two-class variables.\n",
    "\n",
    "Of course, we can use an advanced form of logistic regression, called Multinomial Logistic Regression (not to be confused with Multiple Linear Regression), but the theory for that technique is beyond the scope of this course. Instead, we'll simply use **kNN** here for multi-class classification.\n",
    "\n",
    "Let's go back to the Iris dataset. Your tasks are as follows:\n",
    "\n",
    "1. **Import and explore the data** (`data/IRIS.csv`) so that you're familiar with it (if you're not already).\n",
    "2. As you did with the Titanic dataset, **transform the data** as necessary,\n",
    "3. **split the data** into training and testing (80-20, ensuring that each set is representative of the whole dataset), \n",
    "4. **scale the data** according to the training set distribution, \n",
    "5. **fit a new model** using a new instance of the kNN classifier, and finally **find the training and testing scores** of the model with this data.\n",
    "\n",
    "You could feel free to use the previously-written helper functions for the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa\n",
       "5           5.4          3.9           1.7          0.4  Iris-setosa\n",
       "6           4.6          3.4           1.4          0.3  Iris-setosa\n",
       "7           5.0          3.4           1.5          0.2  Iris-setosa\n",
       "8           4.4          2.9           1.4          0.2  Iris-setosa\n",
       "9           4.9          3.1           1.5          0.1  Iris-setosa"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: import and explore the data\n",
    "iris = pd.read_csv(\"data/IRIS.csv\")\n",
    "iris.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you did with the Titanic dataset, **transform the data** as necessary, **split the data** into training and testing (80-20, ensuring that each set is representative of the whole dataset), **scale the data** according to the training set distribution, **fit a new model** using a new instance of the kNN classifier, and finally **find the training and testing scores** of the model with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set mean: 0.0000 \t train set var: 1.0000 \n",
      "test set mean: 0.2119 \t test set var: 1.0001\n",
      "Training Score: 0.9583333333333334\n",
      "Testing Score:  0.9\n"
     ]
    }
   ],
   "source": [
    "# TODO: transform, split, scale, fit, score\n",
    "# transform label\n",
    "le = LabelEncoder()\n",
    "le.fit(iris[\"species\"])\n",
    "iris[\"species\"] = le.transform(iris[\"species\"])\n",
    "\n",
    "# split data\n",
    "train_x_iris, test_x_iris, train_y_iris, test_y_iris= train_test_split(iris.iloc[:,:-1],iris.iloc[:,-1], test_size=0.2)\n",
    "\n",
    "# scale data\n",
    "ss = StandardScaler()\n",
    "ss.fit(train_x_iris)\n",
    "train_x_iris_scaled = ss.transform(train_x_iris)\n",
    "test_x_iris_scaled = ss.transform(test_x_iris)\n",
    "\n",
    "# means and variances to check\n",
    "print(\"train set mean: {:5.4f} \\t train set var: {:5.4f} \\ntest set mean: {:5.4f} \\t test set var: {:5.4f}\".format(train_x_iris_scaled.mean(), train_x_iris_scaled.var(), test_x_iris_scaled.mean(), test_x_iris_scaled.var()))\n",
    "\n",
    "# convert to DataFrames\n",
    "train_x_iris_scaled = pd.DataFrame(train_x_iris_scaled)\n",
    "train_x_iris_scaled.columns = train_x_iris.columns\n",
    "\n",
    "test_x_iris_scaled = pd.DataFrame(test_x_iris_scaled)\n",
    "test_x_iris_scaled.columns = test_x_iris.columns\n",
    "\n",
    "# fit data\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_iris_model = knn.fit(train_x_iris_scaled, train_y_iris)\n",
    "\n",
    "# find scores\n",
    "print(\"Training Score:\", knn_iris_model.score(train_x_iris_scaled, train_y_iris))\n",
    "print(\"Testing Score: \", knn_iris_model.score(test_x_iris_scaled, test_y_iris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did everything right, you should be getting fairly high scores. Run the code a few times using different, random train-test partitions to get a better understanding of the average score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Confusing You Some More\n",
    "We've explored the default mean accuracy score (using `model.score()`), but classification also has other important scoring techniques that are useful for diagnosing your model. To start off, let's **create a confusion matrix** using <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\">Scikit-Learn's Confusion Matrix module</a>. Using the testing data for the Titanic dataset and the logistic regression model you created, produce a confusion matrix. Ensure that you give the function the right parameter values (use `help(confusion_matrix)` if needed). (Hint: you'll need to use the `model.predict()` function to create `y_pred`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[78  8]\n",
      " [15 42]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: create confusion matrix\n",
    "print(confusion_matrix(test_y, logres_model.predict(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty confusing (pun intended) set of numbers there. What do they mean?\n",
    "\n",
    "The confusion matrix is made up of $n$ columns and $n$ rows, where $n$ is the number of target levels you have (2 for the Titanic dataset). The rows indicate the observations, or actuals, while the columns indicate the predicted, starting from the lowest level. Specifically, if $C$ is the confusion matrix, then $C_{0,0}$ is the **true negatives** (predicted negative, actually negative), $C_{0,1}$ is the **false positives** (predicted positive, actually negative), $C_{1,0}$ is the **false negatives** (predicted negative, actually positive) and $C_{1,1}$ is the **true positives** (predicted positive, actually positive). As we want correct predictions, we want $C_{0,0}$ and $C_{1,1}$ (i.e. values on the main diagonal) to be as large as possible. The documentation for this module also explains this.\n",
    "\n",
    "Re-produce the confusion matrix, but this time save it to four new objects by using `tn, fp, fn, tp = ...` and using the `ravel()` function (the `confusion_matrix()` documentation has an example of this). This will keep a record of each of the four numbers described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: save confusion matrix counts\n",
    "tn, fp, fn, tp = confusion_matrix(test_y, logres_model.predict(test_x)).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a (crude) way of show the confusion matrix counts, their labels, counts and sums; if you've completed the previous steps correctly you should be able to just run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 PREDICTION\n",
      "                __0_____1__\n",
      "OBSERVATION  0 | 78     8 | 86\n",
      "             1 | 15    42 | 57\n",
      "               ------------\n",
      "                 93    50   143\n"
     ]
    }
   ],
   "source": [
    "print(\"                 PREDICTION\")\n",
    "print(\"                __0_____1__\")\n",
    "print(\"OBSERVATION  0 |\", str(tn).rjust(2), \"  \", str(fp).rjust(2), \"|\", tn+fp)\n",
    "print(\"             1 |\", str(fn).rjust(2), \"  \", str(tp).rjust(2), \"|\", fn+tp)\n",
    "print(\"               ------------\")\n",
    "print(\"                \", tn+fn, \"  \", fp+tp, \" \", tn+fp+fn+tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can compare the predictions and observations. If your model is behaving unexpectedly, you can use the confusion matrix to easily determine whether the model is only predicting one label. Confusion matrices are also important if you especially want to avoid a particular type of incorrect prediction. For example, a cancer screening that incorrectly classifies a person as not having cancer when they do have cancer is life-threatening, so you'd want to alter your model to avoid that.\n",
    "\n",
    "Now let's calculate a few different scoring metrics:\n",
    "- **Recall**: TP / (TP + FN). This describes the proportion of actual-positive observations that were correctly classified.\n",
    "- **Precision**: TP / (TP + FP). This is the percentage of positive-predicted observations that were correctly classified.\n",
    "- **Accuracy**: (TP + TN) / (TP+FP+FN+TN). This is the percentage of correctly classified observations in total. This is the same as the `model.score()` function that we used earlier.\n",
    "- **F1**: (2 * Recall * Prediction) / (Recall + Prediction). This is a weighted average of recall and precision, and generally a better metric than accuracy for data that is unbalanced with respect to its target labels.\n",
    "\n",
    "**Find each of the scores above** for your Titanic logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:    0.7368421052631579 \n",
      "Precision: 0.84 \n",
      "Accuracy:  0.8391608391608392 \n",
      "F1 Score:  0.7850467289719626\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate recall, precision, accuracy and F1 scores\n",
    "recall = tp/(tp+fn)\n",
    "prec = tp/(tp+fp)\n",
    "acc = (tp+tn)/(tp+fp+fn+tn)\n",
    "f1 = (2*recall*prec)/(recall+prec)\n",
    "print(\"Recall:   \", recall,\n",
    "    \"\\nPrecision:\", prec,\n",
    "    \"\\nAccuracy: \", acc,\n",
    "    \"\\nF1 Score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look through these scores and understand what they mean for your model. Are these scores fairly similar? If not, how come?\n",
    "\n",
    "Re-fit the kNN model for the Titanic dataset (especially if you've played around with the `n_neighbors` parameter), and repeat the above steps to **calculate the four metrics for the kNN model**. Compare the two models' metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:    0.7017543859649122 \n",
      "Precision: 0.8333333333333334 \n",
      "Accuracy:  0.8251748251748252 \n",
      "F1 Score:  0.7619047619047619\n"
     ]
    }
   ],
   "source": [
    "# TODO: repeat for kNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn_model = knn.fit(train_x_scaled, train_y)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test_y, knn_model.predict(test_x_scaled)).ravel()\n",
    "\n",
    "recall = tp/(tp+fn)\n",
    "prec = tp/(tp+fp)\n",
    "acc = (tp+tn)/(tp+fp+fn+tn)\n",
    "f1 = (2*recall*prec)/(recall+prec)\n",
    "print(\"Recall:   \", recall,\n",
    "    \"\\nPrecision:\", prec,\n",
    "    \"\\nAccuracy: \", acc,\n",
    "    \"\\nF1 Score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to the confusion matrix if you change the `n_neighbors` parameter to be equal to the size of the training data? Fit the kNN model with `n_neighbors=N`, where `N` is the size of the training set, and view the output of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[86  0]\n",
      " [57  0]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: change n_neighbors and look at confusion matrix\n",
    "knn = KNeighborsClassifier(n_neighbors=train_x_scaled.shape[0])\n",
    "knn_model = knn.fit(train_x_scaled, train_y)\n",
    "\n",
    "print(confusion_matrix(test_y, knn_model.predict(test_x_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the output for the confusion matrix matches with your answer to the previous question when you were adjusting the `n_neighbors` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework & Extension Questions\n",
    "You will need to complete previous exercises before starting these exercises.\n",
    "\n",
    "### Exercise 5: Scaled or Un-Scaled?\n",
    "In an earlier exercise, we showed that scaling was necessary for the kNN classifier. Now, for both the Titanic and Iris datasets, **fit new models using un-scaled data** and compare the predictive scores. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSCALED | Training: 0.7796847635726796 \n",
      "         | Testing:  0.6832167832167831\n",
      "  SCALED | Training: 0.8560420315236428 \n",
      "         | Testing:  0.7944055944055944\n"
     ]
    }
   ],
   "source": [
    "# TODO: fit un-scaled kNN model for Titanic and compare\n",
    "\n",
    "# non-scaled\n",
    "unscaled_tr = []\n",
    "unscaled_te = []\n",
    "for _ in range(10):\n",
    "    # split data\n",
    "    train_ttnc, test_ttnc = train_test_split(processed_titanic, test_size=0.2)\n",
    "\n",
    "    # fit data\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_ttnc_model = knn.fit(train_ttnc.iloc[:,:-1], train_ttnc.iloc[:,-1])\n",
    "\n",
    "    # find scores\n",
    "    unscaled_tr.append(knn_ttnc_model.score(train_ttnc.iloc[:,:-1], train_ttnc.iloc[:,-1]))\n",
    "    unscaled_te.append(knn_ttnc_model.score(test_ttnc.iloc[:,:-1], test_ttnc.iloc[:,-1]))\n",
    "    \n",
    "scaled_tr = []\n",
    "scaled_te = []\n",
    "for _ in range(10):\n",
    "    # split data\n",
    "    train_ttnc, test_ttnc = train_test_split(processed_titanic, test_size=0.2)\n",
    "\n",
    "    # scale data\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(train_ttnc)\n",
    "    train_ttnc_scaled = ss.transform(train_ttnc)\n",
    "    test_ttnc_scaled = ss.transform(test_ttnc)\n",
    "\n",
    "    # convert to DataFrames\n",
    "    train_ttnc_scaled = pd.DataFrame(train_ttnc_scaled)\n",
    "    train_ttnc_scaled.columns = train_ttnc.columns\n",
    "    test_ttnc_scaled = pd.DataFrame(test_ttnc_scaled)\n",
    "    test_ttnc_scaled.columns = test_ttnc.columns\n",
    "\n",
    "    # fit data\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_ttnc_model = knn.fit(train_ttnc_scaled.iloc[:,:-1], train_ttnc.iloc[:,-1])\n",
    "\n",
    "    # find scores\n",
    "    scaled_tr.append(knn_ttnc_model.score(train_ttnc_scaled.iloc[:,:-1], train_ttnc.iloc[:,-1]))\n",
    "    scaled_te.append(knn_ttnc_model.score(test_ttnc_scaled.iloc[:,:-1], test_ttnc.iloc[:,-1]))\n",
    "    \n",
    "print(  \"UNSCALED | Training:\", np.mean(unscaled_tr),\n",
    "      \"\\n         | Testing: \", np.mean(unscaled_te))\n",
    "print(  \"  SCALED | Training:\", np.mean(scaled_tr),\n",
    "      \"\\n         | Testing: \", np.mean(scaled_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSCALED | Training: 0.9766666666666666 \n",
      "         | Testing:  0.9466666666666667\n",
      "  SCALED | Training: 0.9674999999999999 \n",
      "         | Testing:  0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "# TODO: fit un-scaled kNN model for Iris and compare\n",
    "\n",
    "# non-scaled\n",
    "unscaled_tr = []\n",
    "unscaled_te = []\n",
    "for _ in range(10):\n",
    "    # split data\n",
    "    train_iris, test_iris = train_test_split(iris, test_size=0.2)\n",
    "\n",
    "    # fit data\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_iris_model = knn.fit(train_iris.iloc[:,:-1], train_iris.iloc[:,-1])\n",
    "\n",
    "    # find scores\n",
    "    unscaled_tr.append(knn_iris_model.score(train_iris.iloc[:,:-1], train_iris.iloc[:,-1]))\n",
    "    unscaled_te.append(knn_iris_model.score(test_iris.iloc[:,:-1], test_iris.iloc[:,-1]))\n",
    "    \n",
    "scaled_tr = []\n",
    "scaled_te = []\n",
    "for _ in range(10):\n",
    "    # split data\n",
    "    train_iris, test_iris = train_test_split(iris, test_size=0.2)\n",
    "\n",
    "    # scale data\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(train_iris)\n",
    "    train_iris_scaled = ss.transform(train_iris)\n",
    "    test_iris_scaled = ss.transform(test_iris)\n",
    "\n",
    "    # convert to DataFrames\n",
    "    train_iris_scaled = pd.DataFrame(train_iris_scaled)\n",
    "    train_iris_scaled.columns = train_iris.columns\n",
    "    test_iris_scaled = pd.DataFrame(test_iris_scaled)\n",
    "    test_iris_scaled.columns = test_iris.columns\n",
    "\n",
    "    # fit data\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_iris_model = knn.fit(train_iris_scaled.iloc[:,:-1], train_iris.iloc[:,-1])\n",
    "\n",
    "    # find scores\n",
    "    scaled_tr.append(knn_iris_model.score(train_iris_scaled.iloc[:,:-1], train_iris.iloc[:,-1]))\n",
    "    scaled_te.append(knn_iris_model.score(test_iris_scaled.iloc[:,:-1], test_iris.iloc[:,-1]))\n",
    "    \n",
    "print(  \"UNSCALED | Training:\", np.mean(unscaled_tr),\n",
    "      \"\\n         | Testing: \", np.mean(unscaled_te))\n",
    "print(  \"  SCALED | Training:\", np.mean(scaled_tr),\n",
    "      \"\\n         | Testing: \", np.mean(scaled_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one of these, you'll find a noticeable improvement in the performance, while the other might be more or less the same as when you used scaled data. Investigate the datasets and **figure out why scaling has a larger impact on one model**. *Hint: look at the descriptive statistics for both datasets. Which statistic(s) are most relevant?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: investigate why scaling depends on dataset\n",
    "We see that the Titanic dataset has much higher differences in variance and range (consider Sex vs Fare) than the Iris dataset. As the Iris dataset is already more-or-less \"scaled\", the difference in ranges of the Iris predictors doesn't affect the kNN algorithm, while the unscaled Titanic dataset would result in a model that treats the Fare predictor as \"too important\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Looking In The Grey Area\n",
    "Logistic regression has another advantage that we haven't mentioned: it can produce a \"reliable\" *probability* of success, rather than a black-and-white \"success or fail\" output. While the kNN classifier can also do this (by comparing the targets of its nearest neighbours), this isn't as reliable and it depends heavily on the `n_neighbors` parameter.\n",
    "\n",
    "Have a look at the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">documentation for the Logistic Regression module</a> again (or use `help(LogisticRegression)`) and find out which function can be used to calculate the probability of success.\n",
    "\n",
    "Then, repeat the prediction for yourself and/or a character, and **report the probability of survival**. If you had to guess some of the predictor values for that person, try altering them slightly and see how it affects the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of survival is 14.35 %\n"
     ]
    }
   ],
   "source": [
    "# TODO: find probability of survival for yourself and/or a character\n",
    "# lower class, male, 20yo, 0 relatives on-board, $10 fare\n",
    "pred = logres_model.predict_proba(np.array([[3, 1, 20, 0, 10]]))\n",
    "print(\"Probability of survival is\", (pred[0][1]*100).round(2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
